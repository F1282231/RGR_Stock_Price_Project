{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGR Stock Price Forecasting Project - Part 3\n",
    "\n",
    "Author: Jack Wang\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Stock prices are hard to predict because they are not only affected by the performance of the underlying companies but also the expectations from the general public. As known, the stock price of firearm companies are highly correlated to the public opinions toward gun control. My model intends to predict the stock price of one of the largest firearm company in the states, RGR (Sturm, Ruger & Co., firearm company), by using its historical stock price, public opinions toward gun control, and its financial reports to SEC. \n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "The goal of my project is to build a **time series regression model** that predicts the stock price of RGR. The data I am using would be historical stock price from [Yahoo Finance](https://finance.yahoo.com/quote/RGR/history?p=RGR), twitter posts scraped from [twitter](https://twitter.com/), subreddit posts mentioned about gun control, and also the financial reports to [SEC](https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000095029&type=&dateb=&owner=exclude&count=100). I will do sentiment analysis on the text data and time series modeling on the historical stock price data. The model will be evaluated using MSE.\n",
    "\n",
    "## Content\n",
    "\n",
    "This project consists of 7 Jupyter notebooks:\n",
    "- Part-1-stock-price-data\n",
    "- Part-2-twitter-scraper\n",
    "- ***Part-3-twitter-data-cleaning***\n",
    "    - [2016 Twitter Data](#2016-Twitter-Data)\n",
    "    - [2017 Twitter Data](#2017-Twitter-Data)\n",
    "    - [2018 Twitter Data](#2018-Twitter-Data)\n",
    "    - [2019 Twitter Data](#2019-Twitter-Data)\n",
    "    - [All Twitter Data](#All-Twitter-Data)\n",
    "- Part-4-reddit-data-scraper\n",
    "- Part-5-reddit-data-cleaning\n",
    "- Part-6-sec-data-cleaning\n",
    "- Part-7-modeling-and-evaluation\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016 Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "df=[]\n",
    "for i in range(10, 13, 1):\n",
    "    df.append(pd.read_csv(f\"../data/twitter/twitter_2016_{i}_{i}.csv\"))    \n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_10_16.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_10_8.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_10_15.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_11_8.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_11_10.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_11_15.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_11_30.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_12_16.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging, dropping duplicates, resetting index, and adding features by using groupby\n",
    "\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,how='outer'), df)\n",
    "\n",
    "df_final['time_stamp'] = pd.to_datetime(df_final['time_stamp']).dt.date\n",
    "\n",
    "df_final = df_final.drop_duplicates()\n",
    "\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "df_final = pd.merge(df_final.groupby(by = 'time_stamp').sum(),pd.merge(df_final.groupby(by = 'time_stamp').count(), df_final.groupby(by = 'time_stamp').mean(), left_index= True, right_index = True), left_index= True, right_index = True)\n",
    "\n",
    "df_final = df_final.drop(columns=['tweet_word_count_x', 'compound_x'])\n",
    "\n",
    "df_final.columns=['tweet_word_count_sum', 'tweet_compound_score_sum', 'tweets_sum', 'tweet_word_count_mean', 'tweet_compound_score_mean']\n",
    "\n",
    "df_final['date'] = df_final.index\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv file\n",
    "df_final.to_csv(\"../data/twitter/twitter_2016.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017 Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df=[]\n",
    "for i in range(1, 13, 1):\n",
    "    df.append(pd.read_csv(f\"../data/twitter/twitter_2017_{i}_{i}.csv\"))\n",
    "    \n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2017_10_9.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2017_11_1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging, dropping duplicates, resetting index, and adding features by using groupby\n",
    "\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,how='outer'), df)\n",
    "\n",
    "df_final['time_stamp'] = pd.to_datetime(df_final['time_stamp']).dt.date\n",
    "\n",
    "df_final = df_final.drop_duplicates()\n",
    "\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "df_final = pd.merge(df_final.groupby(by = 'time_stamp').sum(),pd.merge(df_final.groupby(by = 'time_stamp').count(), df_final.groupby(by = 'time_stamp').mean(), left_index= True, right_index = True), left_index= True, right_index = True)\n",
    "\n",
    "df_final = df_final.drop(columns=['tweet_word_count_x', 'compound_x'])\n",
    "\n",
    "df_final.columns=['tweet_word_count_sum', 'tweet_compound_score_sum', 'tweets_sum', 'tweet_word_count_mean', 'tweet_compound_score_mean']\n",
    "\n",
    "df_final['date'] = df_final.index\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv file\n",
    "df_final.to_csv(\"../data/twitter/twitter_2017.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018 Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "df=[]\n",
    "for i in range(1, 13, 1):\n",
    "    df.append(pd.read_csv(f\"../data/twitter/twitter_2018_{i}_{i}.csv\"))\n",
    "    \n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2018_9_16.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2018_3_16.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2018_3_22.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging, dropping duplicates, resetting index, and adding features by using groupby\n",
    "\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,how='outer'), df)\n",
    "\n",
    "df_final['time_stamp'] = pd.to_datetime(df_final['time_stamp']).dt.date\n",
    "\n",
    "df_final = df_final.drop_duplicates()\n",
    "\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "df_final = pd.merge(df_final.groupby(by = 'time_stamp').sum(),pd.merge(df_final.groupby(by = 'time_stamp').count(), df_final.groupby(by = 'time_stamp').mean(), left_index= True, right_index = True), left_index= True, right_index = True)\n",
    "\n",
    "df_final = df_final.drop(columns=['tweet_word_count_x', 'compound_x'])\n",
    "\n",
    "df_final.columns=['tweet_word_count_sum', 'tweet_compound_score_sum', 'tweets_sum', 'tweet_word_count_mean', 'tweet_compound_score_mean']\n",
    "\n",
    "df_final['date'] = df_final.index\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv file\n",
    "df_final.to_csv(\"../data/twitter/twitter_2018.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2019 Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "df=[]\n",
    "for i in range(1, 10, 1):\n",
    "    df.append(pd.read_csv(f\"../data/twitter/twitter_2019_{i}_{i}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging, dropping duplicates, resetting index, and adding features by using groupby\n",
    "\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,how='outer'), df)\n",
    "\n",
    "df_final['time_stamp'] = pd.to_datetime(df_final['time_stamp']).dt.date\n",
    "\n",
    "df_final = df_final.drop_duplicates()\n",
    "\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "df_final = pd.merge(df_final.groupby(by = 'time_stamp').sum(),pd.merge(df_final.groupby(by = 'time_stamp').count(), df_final.groupby(by = 'time_stamp').mean(), left_index= True, right_index = True), left_index= True, right_index = True)\n",
    "\n",
    "df_final = df_final.drop(columns=['tweet_word_count_x', 'compound_x'])\n",
    "\n",
    "df_final.columns=['tweet_word_count_sum', 'tweet_compound_score_sum', 'tweets_sum', 'tweet_word_count_mean', 'tweet_compound_score_mean']\n",
    "\n",
    "df_final['date'] = df_final.index\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv file\n",
    "df_final.to_csv(\"../data/twitter/twitter_2019.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for i in range(2016, 2020, 1):\n",
    "    df.append(pd.read_csv(f\"../data/twitter/twitter_{i}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = reduce(lambda left,right: pd.merge(left,right,how='outer'), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"../data/twitter/twitter.csv\",index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
