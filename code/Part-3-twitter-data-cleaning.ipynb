{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGR Stock Price Forecasting Project\n",
    "\n",
    "Author: Jack Wang\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Stock prices are hard to predict because they are not only affected by the performance of the underlying companies but also the expectations from the general public. As known, the stock price of firearm companies are highly correlated to the public opinions toward gun ban. My model intends to predict the stock price of one of the largest firearm company in the states, RGR (Sturm, Ruger & Co., firearm company), by using its historical stock price and public opinions toward gun ban. \n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "The goal of my projcet is to build a **time series regression model** that predicts the stock price of RGR. The data I am using would be historical stock price from Yahoo Finance, twitter posts scraped from [twitter](https://twitter.com/), and also the news articles from major news website. I will perform NPL on the text data and time series modeling on the historical stock price data. The model will be evaluated using R^2 score.\n",
    "\n",
    "## Content\n",
    "\n",
    "This project consists of 5 Jupyter notebooks:\n",
    "- Part-1-stock-price-data\n",
    "- Part-2-twitter-scraper\n",
    "- ***Part-3-twitter-data-cleaning***\n",
    "- Part-4-reddit-data-scraper\n",
    "- Part-5-reddit-data-cleaning\n",
    "- Part-4-combined-data-and-EDA\n",
    "- Part-5-modeling\n",
    "    - [Example](#Most-Frequent-Words-in-Title-and-Content)\n",
    "- Part-6-Conclusion-and-Discussion\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2016 Twitter Data Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for i in range(10, 13, 1):\n",
    "    df.append(pd.read_csv(f\"../data/twitter/twitter_2016_{i}_{i}.csv\"))\n",
    "    \n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_10_16.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_10_8.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_10_15.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_11_8.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_11_10.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_11_15.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_11_30.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2016_12_16.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = reduce(lambda left,right: pd.merge(left,right,how='outer'), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['time_stamp'] = pd.to_datetime(df_final['time_stamp']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_final.groupby(by = 'time_stamp').sum(),pd.merge(df_final.groupby(by = 'time_stamp').count(), df_final.groupby(by = 'time_stamp').mean(), left_index= True, right_index = True), left_index= True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(columns=['tweet_word_count_x', 'compound_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns=['tweet_word_count_sum', 'tweet_compound_score_sum', 'tweets_sum', 'tweet_word_count_mean', 'tweet_compound_score_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['date'] = df_final.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_word_count_sum</th>\n",
       "      <th>tweet_compound_score_sum</th>\n",
       "      <th>tweets_sum</th>\n",
       "      <th>tweet_word_count_mean</th>\n",
       "      <th>tweet_compound_score_mean</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_stamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>9314</td>\n",
       "      <td>-223.6686</td>\n",
       "      <td>593</td>\n",
       "      <td>15.706577</td>\n",
       "      <td>-0.377181</td>\n",
       "      <td>2016-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>10535</td>\n",
       "      <td>-225.3380</td>\n",
       "      <td>644</td>\n",
       "      <td>16.358696</td>\n",
       "      <td>-0.349904</td>\n",
       "      <td>2016-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>18548</td>\n",
       "      <td>-401.5402</td>\n",
       "      <td>1133</td>\n",
       "      <td>16.370697</td>\n",
       "      <td>-0.354404</td>\n",
       "      <td>2016-10-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-10-04</td>\n",
       "      <td>16554</td>\n",
       "      <td>-356.2059</td>\n",
       "      <td>1017</td>\n",
       "      <td>16.277286</td>\n",
       "      <td>-0.350252</td>\n",
       "      <td>2016-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-10-05</td>\n",
       "      <td>19727</td>\n",
       "      <td>-338.7394</td>\n",
       "      <td>1194</td>\n",
       "      <td>16.521776</td>\n",
       "      <td>-0.283701</td>\n",
       "      <td>2016-10-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-27</td>\n",
       "      <td>11293</td>\n",
       "      <td>-270.1177</td>\n",
       "      <td>719</td>\n",
       "      <td>15.706537</td>\n",
       "      <td>-0.375685</td>\n",
       "      <td>2016-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-28</td>\n",
       "      <td>11157</td>\n",
       "      <td>-232.3530</td>\n",
       "      <td>705</td>\n",
       "      <td>15.825532</td>\n",
       "      <td>-0.329579</td>\n",
       "      <td>2016-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-29</td>\n",
       "      <td>8275</td>\n",
       "      <td>-196.1012</td>\n",
       "      <td>526</td>\n",
       "      <td>15.731939</td>\n",
       "      <td>-0.372816</td>\n",
       "      <td>2016-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>7970</td>\n",
       "      <td>-167.9614</td>\n",
       "      <td>492</td>\n",
       "      <td>16.199187</td>\n",
       "      <td>-0.341385</td>\n",
       "      <td>2016-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>6362</td>\n",
       "      <td>-161.9800</td>\n",
       "      <td>412</td>\n",
       "      <td>15.441748</td>\n",
       "      <td>-0.393155</td>\n",
       "      <td>2016-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tweet_word_count_sum  tweet_compound_score_sum  tweets_sum  \\\n",
       "time_stamp                                                               \n",
       "2016-10-01                  9314                 -223.6686         593   \n",
       "2016-10-02                 10535                 -225.3380         644   \n",
       "2016-10-03                 18548                 -401.5402        1133   \n",
       "2016-10-04                 16554                 -356.2059        1017   \n",
       "2016-10-05                 19727                 -338.7394        1194   \n",
       "...                          ...                       ...         ...   \n",
       "2016-12-27                 11293                 -270.1177         719   \n",
       "2016-12-28                 11157                 -232.3530         705   \n",
       "2016-12-29                  8275                 -196.1012         526   \n",
       "2016-12-30                  7970                 -167.9614         492   \n",
       "2016-12-31                  6362                 -161.9800         412   \n",
       "\n",
       "            tweet_word_count_mean  tweet_compound_score_mean        date  \n",
       "time_stamp                                                                \n",
       "2016-10-01              15.706577                  -0.377181  2016-10-01  \n",
       "2016-10-02              16.358696                  -0.349904  2016-10-02  \n",
       "2016-10-03              16.370697                  -0.354404  2016-10-03  \n",
       "2016-10-04              16.277286                  -0.350252  2016-10-04  \n",
       "2016-10-05              16.521776                  -0.283701  2016-10-05  \n",
       "...                           ...                        ...         ...  \n",
       "2016-12-27              15.706537                  -0.375685  2016-12-27  \n",
       "2016-12-28              15.825532                  -0.329579  2016-12-28  \n",
       "2016-12-29              15.731939                  -0.372816  2016-12-29  \n",
       "2016-12-30              16.199187                  -0.341385  2016-12-30  \n",
       "2016-12-31              15.441748                  -0.393155  2016-12-31  \n",
       "\n",
       "[92 rows x 6 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv file\n",
    "df_final.to_csv(\"../data/twitter/twitter_2016.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017 Twitter Data Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for i in range(1, 13, 1):\n",
    "    df.append(pd.read_csv(f\"../data/twitter/twitter_2017_{i}_{i}.csv\"))\n",
    "    \n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2017_10_9.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2017_11_1.csv\"))\n",
    "#df.append(pd.read_csv(f\"../data/twitter/twitter_2018_3_22.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = reduce(lambda left,right: pd.merge(left,right,how='outer'), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['time_stamp'] = pd.to_datetime(df_final['time_stamp']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_final.groupby(by = 'time_stamp').sum(),pd.merge(df_final.groupby(by = 'time_stamp').count(), df_final.groupby(by = 'time_stamp').mean(), left_index= True, right_index = True), left_index= True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(columns=['tweet_word_count_x', 'compound_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns=['tweet_word_count_sum', 'tweet_compound_score_sum', 'tweets_sum', 'tweet_word_count_mean', 'tweet_compound_score_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['date'] = df_final.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_word_count_sum</th>\n",
       "      <th>tweet_compound_score_sum</th>\n",
       "      <th>tweets_sum</th>\n",
       "      <th>tweet_word_count_mean</th>\n",
       "      <th>tweet_compound_score_mean</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_stamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>7190</td>\n",
       "      <td>-170.7131</td>\n",
       "      <td>471</td>\n",
       "      <td>15.265393</td>\n",
       "      <td>-0.362448</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>18204</td>\n",
       "      <td>-364.8603</td>\n",
       "      <td>1179</td>\n",
       "      <td>15.440204</td>\n",
       "      <td>-0.309466</td>\n",
       "      <td>2017-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>13661</td>\n",
       "      <td>-269.7893</td>\n",
       "      <td>823</td>\n",
       "      <td>16.599028</td>\n",
       "      <td>-0.327812</td>\n",
       "      <td>2017-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>16702</td>\n",
       "      <td>-388.7950</td>\n",
       "      <td>1021</td>\n",
       "      <td>16.358472</td>\n",
       "      <td>-0.380798</td>\n",
       "      <td>2017-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>11556</td>\n",
       "      <td>-252.5025</td>\n",
       "      <td>722</td>\n",
       "      <td>16.005540</td>\n",
       "      <td>-0.349726</td>\n",
       "      <td>2017-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>12771</td>\n",
       "      <td>-180.3469</td>\n",
       "      <td>504</td>\n",
       "      <td>25.339286</td>\n",
       "      <td>-0.357831</td>\n",
       "      <td>2017-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>15430</td>\n",
       "      <td>-200.0833</td>\n",
       "      <td>626</td>\n",
       "      <td>24.648562</td>\n",
       "      <td>-0.319622</td>\n",
       "      <td>2017-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>16841</td>\n",
       "      <td>-247.6907</td>\n",
       "      <td>672</td>\n",
       "      <td>25.061012</td>\n",
       "      <td>-0.368587</td>\n",
       "      <td>2017-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>14819</td>\n",
       "      <td>-190.0229</td>\n",
       "      <td>581</td>\n",
       "      <td>25.506024</td>\n",
       "      <td>-0.327062</td>\n",
       "      <td>2017-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>37013</td>\n",
       "      <td>-479.8856</td>\n",
       "      <td>1506</td>\n",
       "      <td>24.577025</td>\n",
       "      <td>-0.318649</td>\n",
       "      <td>2017-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tweet_word_count_sum  tweet_compound_score_sum  tweets_sum  \\\n",
       "time_stamp                                                               \n",
       "2017-01-01                  7190                 -170.7131         471   \n",
       "2017-01-02                 18204                 -364.8603        1179   \n",
       "2017-01-03                 13661                 -269.7893         823   \n",
       "2017-01-04                 16702                 -388.7950        1021   \n",
       "2017-01-05                 11556                 -252.5025         722   \n",
       "...                          ...                       ...         ...   \n",
       "2017-12-27                 12771                 -180.3469         504   \n",
       "2017-12-28                 15430                 -200.0833         626   \n",
       "2017-12-29                 16841                 -247.6907         672   \n",
       "2017-12-30                 14819                 -190.0229         581   \n",
       "2017-12-31                 37013                 -479.8856        1506   \n",
       "\n",
       "            tweet_word_count_mean  tweet_compound_score_mean        date  \n",
       "time_stamp                                                                \n",
       "2017-01-01              15.265393                  -0.362448  2017-01-01  \n",
       "2017-01-02              15.440204                  -0.309466  2017-01-02  \n",
       "2017-01-03              16.599028                  -0.327812  2017-01-03  \n",
       "2017-01-04              16.358472                  -0.380798  2017-01-04  \n",
       "2017-01-05              16.005540                  -0.349726  2017-01-05  \n",
       "...                           ...                        ...         ...  \n",
       "2017-12-27              25.339286                  -0.357831  2017-12-27  \n",
       "2017-12-28              24.648562                  -0.319622  2017-12-28  \n",
       "2017-12-29              25.061012                  -0.368587  2017-12-29  \n",
       "2017-12-30              25.506024                  -0.327062  2017-12-30  \n",
       "2017-12-31              24.577025                  -0.318649  2017-12-31  \n",
       "\n",
       "[365 rows x 6 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv file\n",
    "df_final.to_csv(\"../data/twitter/twitter_2017.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018 Twitter Data Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for i in range(1, 13, 1):\n",
    "    df.append(pd.read_csv(f\"../data/twitter/twitter_2018_{i}_{i}.csv\"))\n",
    "    \n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2018_9_16.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2018_3_16.csv\"))\n",
    "df.append(pd.read_csv(f\"../data/twitter/twitter_2018_3_22.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = reduce(lambda left,right: pd.merge(left,right,how='outer'), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['time_stamp'] = pd.to_datetime(df_final['time_stamp']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_final.groupby(by = 'time_stamp').sum(),pd.merge(df_final.groupby(by = 'time_stamp').count(), df_final.groupby(by = 'time_stamp').mean(), left_index= True, right_index = True), left_index= True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(columns=['tweet_word_count_x', 'compound_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns=['tweet_word_count_sum', 'tweet_compound_score_sum', 'tweets_sum', 'tweet_word_count_mean', 'tweet_compound_score_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['date'] = df_final.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_word_count_sum</th>\n",
       "      <th>tweet_compound_score_sum</th>\n",
       "      <th>tweets_sum</th>\n",
       "      <th>tweet_word_count_mean</th>\n",
       "      <th>tweet_compound_score_mean</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_stamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>22940</td>\n",
       "      <td>-296.1463</td>\n",
       "      <td>893</td>\n",
       "      <td>25.688690</td>\n",
       "      <td>-0.331631</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>23911</td>\n",
       "      <td>-302.8459</td>\n",
       "      <td>918</td>\n",
       "      <td>26.046841</td>\n",
       "      <td>-0.329897</td>\n",
       "      <td>2018-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>18520</td>\n",
       "      <td>-228.6339</td>\n",
       "      <td>730</td>\n",
       "      <td>25.369863</td>\n",
       "      <td>-0.313197</td>\n",
       "      <td>2018-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>17315</td>\n",
       "      <td>-218.5435</td>\n",
       "      <td>677</td>\n",
       "      <td>25.576071</td>\n",
       "      <td>-0.322812</td>\n",
       "      <td>2018-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>15669</td>\n",
       "      <td>-195.2983</td>\n",
       "      <td>593</td>\n",
       "      <td>26.423272</td>\n",
       "      <td>-0.329339</td>\n",
       "      <td>2018-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>41232</td>\n",
       "      <td>-475.3081</td>\n",
       "      <td>1420</td>\n",
       "      <td>29.036620</td>\n",
       "      <td>-0.334724</td>\n",
       "      <td>2018-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>54610</td>\n",
       "      <td>-712.9452</td>\n",
       "      <td>1925</td>\n",
       "      <td>28.368831</td>\n",
       "      <td>-0.370361</td>\n",
       "      <td>2018-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>41195</td>\n",
       "      <td>-488.9873</td>\n",
       "      <td>1397</td>\n",
       "      <td>29.488189</td>\n",
       "      <td>-0.350027</td>\n",
       "      <td>2018-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>46359</td>\n",
       "      <td>-520.1260</td>\n",
       "      <td>1552</td>\n",
       "      <td>29.870490</td>\n",
       "      <td>-0.335133</td>\n",
       "      <td>2018-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>54011</td>\n",
       "      <td>-556.1109</td>\n",
       "      <td>1760</td>\n",
       "      <td>30.688068</td>\n",
       "      <td>-0.315972</td>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tweet_word_count_sum  tweet_compound_score_sum  tweets_sum  \\\n",
       "time_stamp                                                               \n",
       "2018-01-01                 22940                 -296.1463         893   \n",
       "2018-01-02                 23911                 -302.8459         918   \n",
       "2018-01-03                 18520                 -228.6339         730   \n",
       "2018-01-04                 17315                 -218.5435         677   \n",
       "2018-01-05                 15669                 -195.2983         593   \n",
       "...                          ...                       ...         ...   \n",
       "2018-12-27                 41232                 -475.3081        1420   \n",
       "2018-12-28                 54610                 -712.9452        1925   \n",
       "2018-12-29                 41195                 -488.9873        1397   \n",
       "2018-12-30                 46359                 -520.1260        1552   \n",
       "2018-12-31                 54011                 -556.1109        1760   \n",
       "\n",
       "            tweet_word_count_mean  tweet_compound_score_mean        date  \n",
       "time_stamp                                                                \n",
       "2018-01-01              25.688690                  -0.331631  2018-01-01  \n",
       "2018-01-02              26.046841                  -0.329897  2018-01-02  \n",
       "2018-01-03              25.369863                  -0.313197  2018-01-03  \n",
       "2018-01-04              25.576071                  -0.322812  2018-01-04  \n",
       "2018-01-05              26.423272                  -0.329339  2018-01-05  \n",
       "...                           ...                        ...         ...  \n",
       "2018-12-27              29.036620                  -0.334724  2018-12-27  \n",
       "2018-12-28              28.368831                  -0.370361  2018-12-28  \n",
       "2018-12-29              29.488189                  -0.350027  2018-12-29  \n",
       "2018-12-30              29.870490                  -0.335133  2018-12-30  \n",
       "2018-12-31              30.688068                  -0.315972  2018-12-31  \n",
       "\n",
       "[365 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv file\n",
    "df_final.to_csv(\"../data/twitter/twitter_2018.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019 Twitter Data Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for i in range(1, 10, 1):\n",
    "    df.append(pd.read_csv(f\"../data/twitter/twitter_2019_{i}_{i}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = reduce(lambda left,right: pd.merge(left,right,how='outer'), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['time_stamp'] = pd.to_datetime(df_final['time_stamp']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_final.groupby(by = 'time_stamp').sum(),pd.merge(df_final.groupby(by = 'time_stamp').count(), df_final.groupby(by = 'time_stamp').mean(), left_index= True, right_index = True), left_index= True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(columns=['tweet_word_count_x', 'compound_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns=['tweet_word_count_sum', 'tweet_compound_score_sum', 'tweets_sum', 'tweet_word_count_mean', 'tweet_compound_score_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['date'] = df_final.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_word_count_sum</th>\n",
       "      <th>tweet_compound_score_sum</th>\n",
       "      <th>tweets_sum</th>\n",
       "      <th>tweet_word_count_mean</th>\n",
       "      <th>tweet_compound_score_mean</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_stamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>37704</td>\n",
       "      <td>-340.3238</td>\n",
       "      <td>1324</td>\n",
       "      <td>28.477341</td>\n",
       "      <td>-0.257042</td>\n",
       "      <td>2019-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>46399</td>\n",
       "      <td>-498.6116</td>\n",
       "      <td>1604</td>\n",
       "      <td>28.927057</td>\n",
       "      <td>-0.310855</td>\n",
       "      <td>2019-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>41221</td>\n",
       "      <td>-447.3883</td>\n",
       "      <td>1415</td>\n",
       "      <td>29.131449</td>\n",
       "      <td>-0.316175</td>\n",
       "      <td>2019-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>45529</td>\n",
       "      <td>-479.4148</td>\n",
       "      <td>1608</td>\n",
       "      <td>28.314055</td>\n",
       "      <td>-0.298144</td>\n",
       "      <td>2019-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>60166</td>\n",
       "      <td>-757.9812</td>\n",
       "      <td>2155</td>\n",
       "      <td>27.919258</td>\n",
       "      <td>-0.351731</td>\n",
       "      <td>2019-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-09-26</td>\n",
       "      <td>100093</td>\n",
       "      <td>-1024.4333</td>\n",
       "      <td>3372</td>\n",
       "      <td>29.683571</td>\n",
       "      <td>-0.303806</td>\n",
       "      <td>2019-09-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>108463</td>\n",
       "      <td>-1056.7549</td>\n",
       "      <td>3650</td>\n",
       "      <td>29.715890</td>\n",
       "      <td>-0.289522</td>\n",
       "      <td>2019-09-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-09-28</td>\n",
       "      <td>89754</td>\n",
       "      <td>-817.2610</td>\n",
       "      <td>2910</td>\n",
       "      <td>30.843299</td>\n",
       "      <td>-0.280846</td>\n",
       "      <td>2019-09-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-09-29</td>\n",
       "      <td>66931</td>\n",
       "      <td>-679.0312</td>\n",
       "      <td>2166</td>\n",
       "      <td>30.900739</td>\n",
       "      <td>-0.313495</td>\n",
       "      <td>2019-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>71519</td>\n",
       "      <td>-721.8923</td>\n",
       "      <td>2364</td>\n",
       "      <td>30.253384</td>\n",
       "      <td>-0.305369</td>\n",
       "      <td>2019-09-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>273 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tweet_word_count_sum  tweet_compound_score_sum  tweets_sum  \\\n",
       "time_stamp                                                               \n",
       "2019-01-01                 37704                 -340.3238        1324   \n",
       "2019-01-02                 46399                 -498.6116        1604   \n",
       "2019-01-03                 41221                 -447.3883        1415   \n",
       "2019-01-04                 45529                 -479.4148        1608   \n",
       "2019-01-05                 60166                 -757.9812        2155   \n",
       "...                          ...                       ...         ...   \n",
       "2019-09-26                100093                -1024.4333        3372   \n",
       "2019-09-27                108463                -1056.7549        3650   \n",
       "2019-09-28                 89754                 -817.2610        2910   \n",
       "2019-09-29                 66931                 -679.0312        2166   \n",
       "2019-09-30                 71519                 -721.8923        2364   \n",
       "\n",
       "            tweet_word_count_mean  tweet_compound_score_mean        date  \n",
       "time_stamp                                                                \n",
       "2019-01-01              28.477341                  -0.257042  2019-01-01  \n",
       "2019-01-02              28.927057                  -0.310855  2019-01-02  \n",
       "2019-01-03              29.131449                  -0.316175  2019-01-03  \n",
       "2019-01-04              28.314055                  -0.298144  2019-01-04  \n",
       "2019-01-05              27.919258                  -0.351731  2019-01-05  \n",
       "...                           ...                        ...         ...  \n",
       "2019-09-26              29.683571                  -0.303806  2019-09-26  \n",
       "2019-09-27              29.715890                  -0.289522  2019-09-27  \n",
       "2019-09-28              30.843299                  -0.280846  2019-09-28  \n",
       "2019-09-29              30.900739                  -0.313495  2019-09-29  \n",
       "2019-09-30              30.253384                  -0.305369  2019-09-30  \n",
       "\n",
       "[273 rows x 6 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv file\n",
    "df_final.to_csv(\"../data/twitter/twitter_2019.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for i in range(2016, 2020, 1):\n",
    "    df.append(pd.read_csv(f\"../data/twitter/twitter_{i}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = reduce(lambda left,right: pd.merge(left,right,how='outer'), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_word_count_sum</th>\n",
       "      <th>tweet_compound_score_sum</th>\n",
       "      <th>tweets_sum</th>\n",
       "      <th>tweet_word_count_mean</th>\n",
       "      <th>tweet_compound_score_mean</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9314</td>\n",
       "      <td>-223.6686</td>\n",
       "      <td>593</td>\n",
       "      <td>15.706577</td>\n",
       "      <td>-0.377181</td>\n",
       "      <td>2016-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10535</td>\n",
       "      <td>-225.3380</td>\n",
       "      <td>644</td>\n",
       "      <td>16.358696</td>\n",
       "      <td>-0.349904</td>\n",
       "      <td>2016-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>18548</td>\n",
       "      <td>-401.5402</td>\n",
       "      <td>1133</td>\n",
       "      <td>16.370697</td>\n",
       "      <td>-0.354404</td>\n",
       "      <td>2016-10-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>16554</td>\n",
       "      <td>-356.2059</td>\n",
       "      <td>1017</td>\n",
       "      <td>16.277286</td>\n",
       "      <td>-0.350252</td>\n",
       "      <td>2016-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>19727</td>\n",
       "      <td>-338.7394</td>\n",
       "      <td>1194</td>\n",
       "      <td>16.521776</td>\n",
       "      <td>-0.283701</td>\n",
       "      <td>2016-10-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>100093</td>\n",
       "      <td>-1024.4333</td>\n",
       "      <td>3372</td>\n",
       "      <td>29.683571</td>\n",
       "      <td>-0.303806</td>\n",
       "      <td>2019-09-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1091</td>\n",
       "      <td>108463</td>\n",
       "      <td>-1056.7549</td>\n",
       "      <td>3650</td>\n",
       "      <td>29.715890</td>\n",
       "      <td>-0.289522</td>\n",
       "      <td>2019-09-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1092</td>\n",
       "      <td>89754</td>\n",
       "      <td>-817.2610</td>\n",
       "      <td>2910</td>\n",
       "      <td>30.843299</td>\n",
       "      <td>-0.280846</td>\n",
       "      <td>2019-09-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1093</td>\n",
       "      <td>66931</td>\n",
       "      <td>-679.0312</td>\n",
       "      <td>2166</td>\n",
       "      <td>30.900739</td>\n",
       "      <td>-0.313495</td>\n",
       "      <td>2019-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1094</td>\n",
       "      <td>71519</td>\n",
       "      <td>-721.8923</td>\n",
       "      <td>2364</td>\n",
       "      <td>30.253384</td>\n",
       "      <td>-0.305369</td>\n",
       "      <td>2019-09-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1095 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_word_count_sum  tweet_compound_score_sum  tweets_sum  \\\n",
       "0                     9314                 -223.6686         593   \n",
       "1                    10535                 -225.3380         644   \n",
       "2                    18548                 -401.5402        1133   \n",
       "3                    16554                 -356.2059        1017   \n",
       "4                    19727                 -338.7394        1194   \n",
       "...                    ...                       ...         ...   \n",
       "1090                100093                -1024.4333        3372   \n",
       "1091                108463                -1056.7549        3650   \n",
       "1092                 89754                 -817.2610        2910   \n",
       "1093                 66931                 -679.0312        2166   \n",
       "1094                 71519                 -721.8923        2364   \n",
       "\n",
       "      tweet_word_count_mean  tweet_compound_score_mean        date  \n",
       "0                 15.706577                  -0.377181  2016-10-01  \n",
       "1                 16.358696                  -0.349904  2016-10-02  \n",
       "2                 16.370697                  -0.354404  2016-10-03  \n",
       "3                 16.277286                  -0.350252  2016-10-04  \n",
       "4                 16.521776                  -0.283701  2016-10-05  \n",
       "...                     ...                        ...         ...  \n",
       "1090              29.683571                  -0.303806  2019-09-26  \n",
       "1091              29.715890                  -0.289522  2019-09-27  \n",
       "1092              30.843299                  -0.280846  2019-09-28  \n",
       "1093              30.900739                  -0.313495  2019-09-29  \n",
       "1094              30.253384                  -0.305369  2019-09-30  \n",
       "\n",
       "[1095 rows x 6 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"../data/twitter/twitter.csv\",index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
